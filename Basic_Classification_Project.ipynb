{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xt6PXeVjxQN"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2hPzRb6j_CA"
      },
      "source": [
        "# Classification Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xt6PXeVjxQX"
      },
      "source": [
        "In this project you will apply what you have learned about classification and TensorFlow to complete a project from Kaggle. The challenge is to achieve a high accuracy score while trying to predict which passengers survived the Titanic ship crash. After building your model, you will upload your predictions to Kaggle and submit the score that you get."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDzCkRNv8Kmz"
      },
      "source": [
        "## The Titanic Dataset\n",
        "\n",
        "[Kaggle](https://www.kaggle.com) has a [dataset](https://www.kaggle.com/c/titanic/data) containing the passenger list on the Titanic. The data contains passenger features such as age, gender, ticket class, as well as whether or not they survived.\n",
        "\n",
        "Your job is to create a binary classifier using TensorFlow to determine if a passenger survived or not. The `Survived` column lets you know if the person survived. Then, upload your predictions to Kaggle and submit your accuracy score at the end of this Colab, along with a brief conclusion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4IG3YojoVmk"
      },
      "source": [
        "To get the dataset, you'll need to accept the competition's rules by clicking the \"I understand and accept\" button on the [competition rules page](https://www.kaggle.com/c/titanic/rules). Then upload your `kaggle.json` file and run the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeVKtKt9oTmI"
      },
      "outputs": [],
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && cp kaggle.json ~/.kaggle/ && echo 'Done'\n",
        "! kaggle competitions download -c titanic\n",
        "! ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4VxbBaUpnB6"
      },
      "source": [
        "**Note: If you see a \"403 - Forbidden\" error above, you still need to click \"I understand and accept\" on the [competition rules page](https://www.kaggle.com/c/titanic/rules).**\n",
        "\n",
        "Three files are downloaded:\n",
        "\n",
        "1. `train.csv`: training data (contains features and targets)\n",
        "1. `test.csv`: feature data used to make predictions to send to Kaggle\n",
        "1. `gender_submission.csv`: an example competition submission file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U_zk4L_HpWJ"
      },
      "source": [
        "## Step 1: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QzpYWY7o0L4"
      },
      "source": [
        "Perform exploratory data analysis and data preprocessing. Use as many text and code blocks as you need to explore the data. Note any findings. Repair any data issues you find."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhhX2RJEyADd"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Titanic Data and summary statistics\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import kaggle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Extract the train & test CSVs and store them into DataFrames\n",
        "with zipfile.ZipFile('titanic.zip','r') as z:\n",
        "  z.extractall('./')\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Printing the data types of our columns and getting statistics for the DataFrame\n",
        "print(train_df.dtypes, end='\\n-----------------------')\n",
        "train_df.describe()"
      ],
      "metadata": {
        "id": "2OCDC8eLrPY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspecting the Data\n",
        "\n",
        "# Check for missing values in training DataFrame\n",
        "print(\"Training Missing Values:\")\n",
        "print(train_df.isna().sum())\n",
        "\n",
        "# Check for missing values  in testing DataFrame\n",
        "print(\"-----------------------\\nTesting Missing Values:\")\n",
        "print(test_df.isna().sum())"
      ],
      "metadata": {
        "id": "YRhj7R7JtwJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by checking our Training DataFrame for missing values in each column. We find that there are 177 mising Age values, 687 Cabin values, and 2 missing Embarked values. Afterwards, we checking our Testing DataFrame for missing values in each column. We find that there are 86 mising Age values, 1 missing Fare value, and 327 missing Cabin values."
      ],
      "metadata": {
        "id": "Fy_-M-Iwtwj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "wQ4IsHEZGVXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main goals of our Data Processing are to clean our datasets of null values and encode features to allow our model to fit and predict using them. You'll see in the code blocks below, we expand our 12 columns into 69 columns using dummy encoding."
      ],
      "metadata": {
        "id": "h-UaCZzJiRy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both our datasets to simplify preprocessing\n",
        "\n",
        "df = train_df.append(test_df)\n",
        "df.reset_index(inplace=True)\n",
        "df.drop(['index'], inplace=True, axis=1)\n",
        "\n",
        "# Extract the titles from Name and create a new Title Column with mapped titles\n",
        "df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
        "df['Title'] = df['Title'].map({\n",
        "  \"Capt\": \"Officer\",\n",
        "  \"Col\": \"Officer\",\n",
        "  \"Major\": \"Officer\",\n",
        "  \"Jonkheer\": \"Royalty\",\n",
        "  \"Don\": \"Royalty\",\n",
        "  \"Sir\" : \"Royalty\",\n",
        "  \"Dr\": \"Officer\",\n",
        "  \"Rev\": \"Officer\",\n",
        "  \"the Countess\":\"Royalty\",\n",
        "  \"Mme\": \"Mrs\",\n",
        "  \"Mlle\": \"Miss\",\n",
        "  \"Ms\": \"Mrs\",\n",
        "  \"Mr\" : \"Mr\",\n",
        "  \"Mrs\" : \"Mrs\",\n",
        "  \"Miss\" : \"Miss\",\n",
        "  \"Master\" : \"Master\",\n",
        "  \"Lady\" : \"Royalty\"\n",
        "})"
      ],
      "metadata": {
        "id": "Hfa7n1dt2d3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group factors and their respective Age Medians\n",
        "\n",
        "def get_age_median(row):\n",
        "  train_median_df = df[:891].groupby(['Sex', 'Pclass', 'Title']).median().\\\n",
        "    reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n",
        "  conditional = (\n",
        "        (train_median_df['Sex'] == row['Sex']) & \n",
        "        (train_median_df['Title'] == row['Title']) & \n",
        "        (train_median_df['Pclass'] == row['Pclass'])\n",
        "  )\n",
        "  return train_median_df[conditional]['Age'].values[0]\n",
        "\n",
        "df['Age'] = df.apply(lambda row: get_age_median(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n"
      ],
      "metadata": {
        "id": "oVUi_3s32eLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dummy encode our Titles & drop Name Column\n",
        "\n",
        "df.drop('Name', axis=1, inplace=True)\n",
        "\n",
        "titles_dummies = pd.get_dummies(df['Title'], prefix='Title')\n",
        "df = pd.concat([df, titles_dummies], axis=1)\n",
        "df.drop('Title', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "B09r8vM32eiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty Embarked values w/ the Mode (S) and Dummy encode the column\n",
        "\n",
        "df.Embarked.fillna('S', inplace=True)\n",
        "\n",
        "embarked_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
        "df = pd.concat([df, embarked_dummies], axis=1)\n",
        "df.drop('Embarked', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "6Qf3I7Mt47dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set null Cabin to \"X\", set existing values to their 1st letter, and Dummy encode\n",
        "\n",
        "df.Cabin.fillna('X', inplace=True)\n",
        "df['Cabin'] = df['Cabin'].map(lambda c: c[0])\n",
        "\n",
        "cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')    \n",
        "df = pd.concat([df, cabin_dummies], axis=1)\n",
        "df.drop('Cabin', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Za70hujj5r1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy encode Pclass\n",
        "\n",
        "pclass_dummies = pd.get_dummies(df['Pclass'], prefix=\"Pclass\")\n",
        "df = pd.concat([df, pclass_dummies],axis=1)\n",
        "df.drop('Pclass',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "hlyzeClQ7Khx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the ticket prefix and Dummy encode the column \n",
        "\n",
        "def clean_ticket(ticket):\n",
        "  ticket = ticket.replace('.','')\n",
        "  ticket = ticket.replace('/','')\n",
        "  ticket = ticket.split()\n",
        "  ticket = map(lambda t : t.strip(), ticket)\n",
        "  ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
        "  if len(ticket) > 0:\n",
        "      return ticket[0]\n",
        "  else: \n",
        "      return 'XXX'\n",
        "\n",
        "df['Ticket'] = df['Ticket'].map(clean_ticket)\n",
        "\n",
        "tickets_dummies = pd.get_dummies(df['Ticket'], prefix='Ticket')\n",
        "df = pd.concat([df, tickets_dummies], axis=1)\n",
        "df.drop('Ticket', inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "yQLNOe6Y7uPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Family Size column and columns that correspond to groups of sizes\n",
        "\n",
        "df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n",
        "\n",
        "df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
        "df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n",
        "df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5 <= s else 0)"
      ],
      "metadata": {
        "id": "ke_hRoMY9INu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with the average fare\n",
        "df['Fare'].fillna(df.iloc[:891]['Fare'].mean(), inplace=True)\n",
        "\n",
        "# Replace \"male\" with 1 and \"female\" with 0\n",
        "df['Sex'] = df['Sex'].map({\"male\":1, \"female\":0})\n",
        "\n",
        "# Split back into our two DataFrames\n",
        "train_df = df.iloc[:891]\n",
        "test_df = df.iloc[891:]"
      ],
      "metadata": {
        "id": "diHKM7aEEiFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (cont.)"
      ],
      "metadata": {
        "id": "8hNV3FcrGejh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target and feature columns\n",
        "target_column = 'Survived'\n",
        "feature_columns = [c for c in test_df.columns if c not in ['PassengerId','Survived']]\n",
        "orig_columns = ['Survived','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Pclass_1', 'Pclass_2', 'Pclass_3']\n",
        "\n",
        "# Visualize the correlation using heatmaps for both DataFrames\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(train_df[orig_columns].corr(), vmax=0.6, annot=True, square=True, cmap=\"coolwarm\")\n",
        "plt.title('Training Data Column Correlations')\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "orig_columns.remove('Survived')\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(test_df[orig_columns].corr(), vmax=0.6, annot=True, square=True, cmap=\"coolwarm\")\n",
        "plt.title('Testing Data Column Correlations')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rWOr-m7cCtxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmaps above show the correlation between columns for our Training and Testing DataFrames. We can see some **similar patterns** for both sets with relatively strong negative correlations between Lower Classes & Fare and Lower Classes & Age. These are reasonable correlation as people in lower classes (*3rd class) often cannot afford expensive tickets and younger people are more likely to be lower class. We can see the opposite is true for Higher Classes & the mentioned columns as there are strong postive correlations.\n",
        "\n",
        "When comparing Training & Testing correlations, it's important to note **differences**. A significant difference is that there seems to be a much higher correlation between Age and Fare for Testing than for Training."
      ],
      "metadata": {
        "id": "AkUbE3CQrRQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Violin Plot to show the difference in survival based on age & sex\n",
        "fig = plt.figure(figsize=(25, 7))\n",
        "sns.violinplot(x='Sex', y='Age', \n",
        "               hue='Survived', data=train_df, \n",
        "               split=True,\n",
        "               palette={0: \"r\", 1: \"g\"}\n",
        "              );"
      ],
      "metadata": {
        "id": "4V6f4cfKVZh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: 0 = Female, 1 = Male*\n",
        "\n",
        "Based on the Violin Plot above, we can see that women survive more than men (bigger green section). We can also see the distribution of age in the survival for each sex. The age distribution for men seems approximately normal, centered around ~30 but we see a jump in survival for young boys."
      ],
      "metadata": {
        "id": "zft294kWWSu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "axes = sns.factorplot('FamilySize','Survived', \n",
        "                      data=train_df, aspect = 2.5, )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V24Pw3SyXOUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The factorplot above shows us the chances of survival with relation to size of your family (including self). Based on the plot, the peak survival rates occurred with families of size 3-4. It seems that large families had low likelihood of survival with the biggest dropoff happening after 4 family members."
      ],
      "metadata": {
        "id": "vfltyiEAXOlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRiOrGGW6wW6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxmnIepvmdCx"
      },
      "source": [
        "## Step 2: The Model\n",
        "\n",
        "Build, fit, and evaluate a classification model. Perform any model-specific data processing that you need to perform. If the toolkit you use supports it, create visualizations for loss and accuracy improvements. Use as many text and code blocks as you need to explore the data. Note any findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO8x9d6GHwgQ"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKgNoBuEm2h0"
      },
      "outputs": [],
      "source": [
        "# Create a Logistic Regression Model\n",
        "lr = LogisticRegression(max_iter = 100000)\n",
        "\n",
        "# Fit the Model using our training data\n",
        "lr.fit(train_df[feature_columns], train_df[target_column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT46j3S26sE2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXJCSsAdz-f0"
      },
      "source": [
        "## Step 3: Make Predictions and Upload To Kaggle\n",
        "\n",
        "In this step you will make predictions on the features found in the `test.csv` file and upload them to Kaggle using the [Kaggle API](https://github.com/Kaggle/kaggle-api). Use as many text and code blocks as you need to explore the data. Note any findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fijeUn4tIFCo"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEWZUCnT9UkK"
      },
      "outputs": [],
      "source": [
        "# Make a prediction using the testing input data\n",
        "lr_pred = lr.predict(test_df[feature_columns]).astype(int)\n",
        "\n",
        "# Create a new DataFrame with our prediction for submission and convert it to a CSV\n",
        "submission_df = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':lr_pred})\n",
        "submission_df.to_csv('submission.csv',index=False)\n",
        "\n",
        "# Upload the CSV to Kaggle\n",
        "# !kaggle competitions submit -c titanic -f submission.csv -m \"Default LR\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRZni-CBVjFV"
      },
      "source": [
        "What was your Kaggle score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjN-tBAP6kM7"
      },
      "source": [
        ">0.77511"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSw1rDKv6nOO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KdtnUJP2Uen"
      },
      "source": [
        "## Step 4: Iterate on Your Model\n",
        "\n",
        "In this step you're encouraged to play around with your model settings and to even try different models. See if you can get a better score. Use as many text and code blocks as you need to explore the data. Note any findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5rYKVkgT8NL"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTREME GRADIENT BOOSTING\n",
        "\n",
        "xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=1, gpu_id=0,\n",
        "              importance_type='gain', interaction_constraints='',\n",
        "              learning_rate=0.03, max_delta_step=0, max_depth=8, min_child_weight=1, \n",
        "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
        "              tree_method='exact', validate_parameters=1, verbosity=0)\n",
        "\n",
        "# Fit the Model using our training data & make a prediction using the testing input data\n",
        "xgb.fit(train_df[feature_columns], train_df[target_column])\n",
        "\n",
        "# Create a dataframe and plot of the importance of each feature\n",
        "features = pd.DataFrame()\n",
        "features['feature'] = feature_columns\n",
        "features['importance'] = xgb.feature_importances_\n",
        "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
        "features.set_index('feature', inplace=True)\n",
        "features[-20:].plot(kind='barh', figsize=(15, 15))\n",
        "\n",
        "xg_pred = xgb.predict(test_df[feature_columns]).astype(int)\n",
        "\n",
        "# Create a new DataFrame with our prediction for submission and convert it to a CSV\n",
        "sub_df = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':xg_pred})\n",
        "sub_df.to_csv('submission.csv',index=False)\n",
        "\n",
        "# Upload the CSV to Kaggle\n",
        "# !kaggle competitions submit -c titanic -f submission.csv -m \"XGB\""
      ],
      "metadata": {
        "id": "T3QKc21Yb6y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> Kaggle score:  0.76555\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KjmTg2V5b9-D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDEErS0f8oi7"
      },
      "outputs": [],
      "source": [
        "# RANDOM FOREST CLASSIFIER\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, min_samples_leaf=3, max_features=.5, n_jobs=-1)\n",
        "rf.fit(train_df[feature_columns], train_df[target_column])\n",
        "\n",
        "# Create a dataframe and plot of the importance of each feature\n",
        "features = pd.DataFrame()\n",
        "features['feature'] = feature_columns\n",
        "features['importance'] = rf.feature_importances_\n",
        "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
        "features.set_index('feature', inplace=True)\n",
        "features[-20:].plot(kind='barh', figsize=(15, 15))\n",
        "\n",
        "rf_pred = rf.predict(test_df[feature_columns]).astype(int)\n",
        "\n",
        "# Create a new DataFrame with our prediction for submission and convert it to a CSV\n",
        "submission_df = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':rf_pred})\n",
        "submission_df.to_csv('submission.csv',index=False)\n",
        "\n",
        "# Upload the CSV to Kaggle\n",
        "# !kaggle competitions submit -c titanic -f submission.csv -m \"RFC\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Kaggle Score: 0.77751\n"
      ],
      "metadata": {
        "id": "ZdFdUJw06tF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DECISION TREE CLASSIFIER\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "params = [{'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}]\n",
        "b_clf=BaggingClassifier(GridSearchCV(DecisionTreeClassifier(random_state=42),params,cv=3,verbose=1),n_estimators=1000,max_samples=100,bootstrap=True,n_jobs=-1)\n",
        "b_clf.fit(train_df[feature_columns],train_df[target_column])\n",
        "y_pred=b_clf.predict(test_df[feature_columns]).astype(int)\n",
        "\n",
        "# Create a new DataFrame with our prediction for submission and convert it to a CSV\n",
        "sub_df = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':y_pred})\n",
        "sub_df.to_csv('submission.csv',index=False)\n",
        "\n",
        "# Upload the CSV to Kaggle\n",
        "# !kaggle competitions submit -c titanic -f submission.csv -m \"BC + Dtree\""
      ],
      "metadata": {
        "id": "DSKCuIHTvK_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Kaggle score: 0.78708\n",
        "\n"
      ],
      "metadata": {
        "id": "cXhpvtEBCQDQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkyT_lz6_K7E"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "copyright",
        "xSJsNKccqC15",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1"
      ],
      "name": "Basic Classification Project",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}